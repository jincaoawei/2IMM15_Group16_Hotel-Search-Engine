{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vHRVGHt76BYl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Tf-idf matrix compute finished.\n",
      "preper step is finish\n"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "import tkinter as tk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import csv\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import tkinter.scrolledtext as tkscrolled\n",
    "  \n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# Global Varaiables\n",
    "dict1 ={}\n",
    "TopNr = 10\n",
    "data = []\n",
    "posindex = []\n",
    "#td-idf term-document matrix\n",
    "matrix = []\n",
    "#term list from td-idf term-document matrix\n",
    "tml = []\n",
    "#hotol name with index\n",
    "namelist = []\n",
    "#the hyperlink of result\n",
    "link = \" \"\n",
    "m = []\n",
    "\n",
    "#Read info from csv files and compute tf-idf matrix from dataset\n",
    "def compute_dataset():\n",
    "    global matrix\n",
    "    global namelist\n",
    "    global tml\n",
    "    global m\n",
    "    csv_file=open('pr_result.csv') \n",
    "    csv_reader_lines = csv.reader(csv_file)\n",
    "    next(csv_reader_lines)\n",
    "    csv.field_size_limit(100000000)\n",
    "    for one_line in csv_reader_lines:\n",
    "        terms = one_line[1]\n",
    "        hotel = one_line[0]\n",
    "        m.append(terms)\n",
    "        namelist.append(hotel)\n",
    "    csv_file.close()\n",
    "    vectorizer = TfidfVectorizer(use_idf = TRUE, stop_words=\"english\")\n",
    "    matrix = vectorizer.fit_transform(m)\n",
    "    tml = vectorizer.get_feature_names()\n",
    "    read_file('total_info.csv')\n",
    "    print(\"Tf-idf matrix compute finished.\")\n",
    "    \n",
    "\n",
    "# get top words, otherHotels, and median review score for each cluster\n",
    "def getClusterInfo(hotelName, city, clusterGroup, k):\n",
    "    clusterGroup = int(clusterGroup)\n",
    "    \n",
    "    #### reading initial dataset ######\n",
    "    \n",
    "    #load group_hotel\n",
    "    group_hotel = pd.read_pickle(\"./group_hotel.pkl\")\n",
    "    \n",
    "      \n",
    "    ##### filter the dataframe based on city #####\n",
    "\n",
    "    #filter dataframe by hotel city .reset_index(drop=True) is optional\n",
    "    cityDataframe = group_hotel[group_hotel['Hotel_City']==city].reset_index(drop=True)\n",
    "\n",
    "\n",
    "    ###### get models used for kmeans ######\n",
    "    \n",
    "    #add more stop words based on the dataset\n",
    "    useless_words = text.ENGLISH_STOP_WORDS.union([\"room\",\"rooms\",\"run\",\"st\",\"couldn\",\"location\",\"wasn\",\"hotel\",\"stayed\",\"place\",\"best\",\"positive\",\"negative\",\"time\",\"big\",\"minutes\",\"did\",\"day\",\"didn\",\"near\",\"area\",\"staff\",\"city\",\"close\",\"free\",\"service\",\"really\",\"like\",\"stay\",\"bed\",\"beds\",\"bit\",\"es\",\"just\",\"little\",\"night\"])\n",
    "\n",
    "    # for each hotel, the 'all_reviews' cell is changed to a row of terms corresponding to all terms in all rows of 'all_reviews'\n",
    "    vectorizer = TfidfVectorizer(stop_words=useless_words, use_idf=True, smooth_idf=True) #create vectorizer function which make useful words into tokens\n",
    "    tfidfMatrix = vectorizer.fit_transform(cityDataframe['all_reviews'])   #create a (hotel,term) matrix\n",
    "\n",
    "       \n",
    "    #'all_reviews' are clustered based on the terms contained\n",
    "    model = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=1)\n",
    "    model.fit(tfidfMatrix)\n",
    "    \n",
    "    \n",
    "    clusters = model.labels_.tolist()\n",
    "    cityDataframe['Clusters'] = clusters\n",
    "    \n",
    "        \n",
    "    ######## get top 20 words #######\n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "    terms = vectorizer.get_feature_names()\n",
    "\n",
    "    \n",
    "    topWords = []\n",
    "    for index in order_centroids[clusterGroup, :20]:   #for each value(index) of this cluster's first 2o values\n",
    "        topWords.append(' %s' % terms[index])   \n",
    "        \n",
    "            \n",
    "    ##### other hotels in same cluster ######\n",
    "    if city == 'Amsterdam':\n",
    "        totalClusters = 12\n",
    "    elif city == 'Barcelona':\n",
    "        totalClusters = 14\n",
    "    elif city == 'London':\n",
    "        totalClusters = 11\n",
    "    elif city == 'Milan':\n",
    "        totalClusters = 16\n",
    "    elif city == 'Paris':\n",
    "        totalClusters = 13\n",
    "    elif city == 'Vienna':\n",
    "        totalClusters = 13\n",
    "        \n",
    "    loadedDF = pd.read_pickle(\"./%s_%dClusters.pkl\" %(city, totalClusters))\n",
    "    otherHotels = loadedDF[clusterGroup].loc[loadedDF[clusterGroup] != hotelName].loc[loadedDF[clusterGroup].notnull()].reset_index(drop=True)\n",
    "    \n",
    "       \n",
    "    #### median hotel rating score of this cluster ####\n",
    "    medianScore = cityDataframe['Average_Score'].groupby(cityDataframe['Clusters']).median().loc[clusterGroup]\n",
    "    \n",
    "        \n",
    "    return topWords, otherHotels, medianScore    #(list, pandas series, numpy float)\n",
    "    \n",
    "\n",
    "    \n",
    "#read csv file\n",
    "def read_file(name):\n",
    "    global data\n",
    "    csv_file=open(name) \n",
    "    csv_reader_lines = csv.reader(csv_file)\n",
    "    next(csv_reader_lines)\n",
    "    csv.field_size_limit(100000000)\n",
    "    for one_line in csv_reader_lines:\n",
    "        data.append(one_line)\n",
    "    csv_file.close()\n",
    "    \n",
    "# Get input query from user\n",
    "def handle_input(sender):\n",
    "    listNodes.delete(0, END)\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    lancaster_stemmer = LancasterStemmer()\n",
    "    query = sender.get()\n",
    "    terms = []\n",
    "    st = \"\"\n",
    "    for i in range(0, len(query)):\n",
    "        if query[i] != \" \":\n",
    "            st = st + str(query[i])\n",
    "        else:\n",
    "            stem = lemmatizer.lemmatize(st)\n",
    "            st = lancaster_stemmer.stem(stem)\n",
    "            terms.append(st)\n",
    "            st = \"\"\n",
    "        if i == len(query)-1:\n",
    "            stem = lemmatizer.lemmatize(st)\n",
    "            st = lancaster_stemmer.stem(stem)\n",
    "            terms.append(st)\n",
    "    #print(terms)\n",
    "    count = 0\n",
    "    pos = []\n",
    "    for token in terms:\n",
    "        for term in tml:\n",
    "            if token == term:\n",
    "                pos.append(count)\n",
    "                count = 0\n",
    "            else:\n",
    "                count = count + 1\n",
    "            if count == len(tml) - 1:\n",
    "                pos.append(-1)\n",
    "                count = 0\n",
    "    grade = []\n",
    "    xlist = matrix.toarray()\n",
    "    for one_pos in pos:\n",
    "        if one_pos != -1:\n",
    "            one_pos_grade = [row[one_pos] for row in xlist]\n",
    "            grade.append(one_pos_grade)\n",
    "    total_grade = np.sum(grade, axis=0)\n",
    "    final = total_grade.argsort()[::-1]\n",
    "    for i in namelist:\n",
    "        for t in terms:\n",
    "            if t == i:\n",
    "                listNodes.insert(END, i)\n",
    "    listRank.delete(0, END)\n",
    "    if np.all(final==0):\n",
    "        listRank.insert(END, \"No ranking hotel found\")\n",
    "    else:\n",
    "        for i in range(0,TopNr):\n",
    "            index = final[i]\n",
    "            listRank.insert(END, namelist[index])\n",
    "    \n",
    "    result_Boolean=boolean_search(terms)\n",
    "    if len(result_Boolean)== 0 :\n",
    "        listNodes.insert(END, 'Wrong input keyword, or nothing is input')\n",
    "    else:\n",
    "        listNodes.insert(END, 'Boolean IR search result')\n",
    "        for w in result_Boolean:\n",
    "            listNodes.insert(END,w[0])   \n",
    "\n",
    "\n",
    "# Search button click event    \n",
    "def button_click():\n",
    "    clusterResult.delete(0, END)\n",
    "    handle_input(inp)\n",
    "\n",
    "    \n",
    "def reSelect(event):\n",
    "    w = event.widget\n",
    "    try:\n",
    "        index = int(w.curselection()[0])    #index stores the index of the first thing from the listbox that was selected\n",
    "    except IndexError:\n",
    "        return\n",
    "    value = w.get(index)\n",
    "    listSelection.delete(0, END)\n",
    "    for h in data:\n",
    "        if value == h[0]:\n",
    "            listSelection.insert(END, \"Address: \" + h[1])\n",
    "            listSelection.insert(END, \"Average Score: \" + h[3])\n",
    "            listSelection.insert(END, \"Total Reviews: \" + h[5])\n",
    "            listSelection.insert(END, \"Total Positive Comments:\" + h[6])\n",
    "            listSelection.insert(END, \"Total Negative Comments:\" + h[4])\n",
    "    if len(listSelection.get(0, END)) == 0:                              #if there is no hotel listed in the listbox\n",
    "        listSelection.insert(END, \"Error: Can not find data\")\n",
    "    \n",
    "    listreview.delete(\"1.0\", END)\n",
    "    inc = 0\n",
    "    for hn in namelist:\n",
    "        if value == hn:\n",
    "            old = m[inc]\n",
    "            new = old.split()[:2000]\n",
    "            listreview.insert(tk.INSERT, new)\n",
    "        inc = inc + 1\n",
    "    \n",
    "    \n",
    "def curSelect(event):\n",
    "    #find the listbox widget that is calling for this function\n",
    "    caller = event.widget\n",
    "    selectedIndex = caller.curselection()\n",
    "    if selectedIndex:\n",
    "        selectedName = caller.get(selectedIndex[0])    #select hotel name stored\n",
    "    \n",
    "    #find city of the hotel using group_hotel dataframe\n",
    "    group_hotel = pd.read_pickle('group_hotel.pkl')\n",
    "    city = group_hotel[group_hotel['Hotel_Name']==selectedName]['Hotel_City'].to_string(index=False)   #return the city in string\n",
    "    \n",
    "    #load the pkl of the city to find clusterGroup\n",
    "    if city == 'Amsterdam':\n",
    "        totalClusters = 12\n",
    "    elif city == 'Barcelona':\n",
    "        totalClusters = 14\n",
    "    elif city == 'London':\n",
    "        totalClusters = 11\n",
    "    elif city == 'Milan':\n",
    "        totalClusters = 16\n",
    "    elif city == 'Paris':\n",
    "        totalClusters = 13\n",
    "    elif city == 'Vienna':\n",
    "        totalClusters = 13\n",
    "        \n",
    "    cityPkl = pd.read_pickle(\"./%s_%dClusters.pkl\" %(city, totalClusters))\n",
    "    clusterGroup = cityPkl[cityPkl==selectedName].stack().to_string().split()[1]   #clusterGroup returned as string\n",
    "    \n",
    "        \n",
    "    topWords, otherHotels, medianScore = getClusterInfo(selectedName, city, clusterGroup, totalClusters)\n",
    "    \n",
    "    clusterResult.delete(0, END)\n",
    "    \n",
    "    clusterResult.insert(END, 'Cluster Information:\\n')\n",
    "    clusterResult.insert(END, \"\\n\")\n",
    "    clusterResult.insert(END, \"Selected Hotel: \" + selectedName + \"\\n\")\n",
    "    clusterResult.insert(END, \"\\n\")\n",
    "    clusterResult.insert(END, \"Hotel City: \" + city + \"\\n\")\n",
    "    clusterResult.insert(END, \"\\n\")\n",
    "    clusterResult.insert(END, \"Hotel Cluster Group: \" + clusterGroup + \"\\n\")\n",
    "    clusterResult.insert(END, \"\\n\")\n",
    "    clusterResult.insert(END, \"The Median Review Score for this Cluster Group is: \" + str(medianScore) +\"\\n\")\n",
    "    clusterResult.insert(END, \"\\n\")\n",
    "    clusterResult.insert(END, \"Top 20 Words of This Cluster:\\n\")\n",
    "    for word in topWords:\n",
    "        clusterResult.insert(END, word + \"\\n\")\n",
    "    clusterResult.insert(END, \"\\n\")\n",
    "    clusterResult.insert(END, \"Hotels in the Same Cluster Group:\\n\")\n",
    "    for i in otherHotels:\n",
    "        clusterResult.insert(END, i + \"\\n\")\n",
    "\n",
    "def create_dictionary(file_name):\n",
    "    global dict1\n",
    "\n",
    "    csv.field_size_limit(100000000)\n",
    "    csv_file=open(file_name)    \n",
    "    csv_reader_lines = csv.reader(csv_file)   \n",
    "    date=[]    \n",
    "    renshu = 0\n",
    "    for one_line in csv_reader_lines:\n",
    "        date.append(one_line)    \n",
    "        renshu = renshu + 1  \n",
    "\n",
    "    pre = []\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    for i in range(1,len(date)):\n",
    "       name = date[i][0]\n",
    "       review = date[i][1]\n",
    "       word_tokens = word_tokenize(review) \n",
    "       filtered_sentence = [] \n",
    "       for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w) \n",
    "       f_review = [name, filtered_sentence]\n",
    "       pre.append(f_review)\n",
    "        \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    lemmatizer.lemmatize(\"rocks\")\n",
    "    pre4 = pre\n",
    "    for i in range (0,len(pre)):\n",
    "        data = pre4[i][1]\n",
    "        data2=data\n",
    "        for j in range(0,len(data)):\n",
    "            data3 = data[j]\n",
    "            data_lem = lemmatizer.lemmatize(data3)\n",
    "            data2[j] = data_lem\n",
    "        pre4[i][1] = data2\n",
    "   \n",
    "    lancaster_stemmer = LancasterStemmer()\n",
    "    pre2 = pre4\n",
    "    \n",
    "    for i in range (0,len(pre4)):\n",
    "        dat = pre2[i][1]\n",
    "        dat2=dat\n",
    "        for j in range(0,len(dat)):\n",
    "            dat3 = dat[j]\n",
    "            data_stem = lancaster_stemmer.stem(dat3)\n",
    "            dat2[j] = data_stem\n",
    "        pre2[i][1] = dat2\n",
    "    \n",
    "\n",
    "    a = \"and\"\n",
    "    for i in range (0, len(pre2)):    \n",
    "        li = pre2[i][1]\n",
    "        name = pre2[i][0]\n",
    "        for w in li:\n",
    "            if w != a:\n",
    "                if w not in dict1:\n",
    "                    dict2={}\n",
    "                    dict2[name] = 1\n",
    "                    dict1[w] = dict2\n",
    "                else:\n",
    "                    p= dict1[w]\n",
    "                    if name not in p:\n",
    "                        p[name] = 1\n",
    "                        dict1[w] = p\n",
    "                    else:\n",
    "                        f = p[name]\n",
    "                        p[name] = f+1\n",
    "                        dict1[w] = p \n",
    "\n",
    "    print(\"dictionary step is finish\")\n",
    "  \n",
    "      \n",
    "def boolean_search(input_t):\n",
    "    input_txt = []\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    lancaster_stemmer = LancasterStemmer()\n",
    "    \n",
    "    for w in input_t: \n",
    "        if w not in stop_words: \n",
    "            lem = lemmatizer.lemmatize(w)\n",
    "            stem = lancaster_stemmer.stem(lem)\n",
    "            input_txt.append(stem) \n",
    "    \n",
    "    AND = \"and\"\n",
    "    OR = \"or\"\n",
    "    #NOT = \"not\"\n",
    "    case = 5\n",
    "    for i in input_txt:\n",
    "        if i in dict1.keys():\n",
    "            case = 0\n",
    "    if case == 0:\n",
    "        if AND in input_txt:\n",
    "            case = 1\n",
    "            process = {}\n",
    "            a = 0 \n",
    "            for i in input_txt:\n",
    "                if i in dict1.keys():\n",
    "                    l = dict1[i]\n",
    "                    process[a] = l\n",
    "                    a = a+1\n",
    "        elif OR in input_txt:\n",
    "            case = 2\n",
    "            process = {}\n",
    "            a = 0 \n",
    "            for i in input_txt:\n",
    "                if i in dict1.keys():\n",
    "                    l = dict1[i]\n",
    "                    process[a] = l\n",
    "                    a = a+1\n",
    "        else:\n",
    "            case = 3\n",
    "            process = {}\n",
    "            a = 0 \n",
    "            for i in input_txt:\n",
    "                if i in dict1.keys():\n",
    "                    l = dict1[i]\n",
    "                    process[a] = l\n",
    "                    a = a+1\n",
    "\n",
    "    if case == 5:\n",
    "        pr = {}\n",
    "           \n",
    "    if case == 1 or case == 3:\n",
    "        pr = process[0]\n",
    "        for z in range(1,len(process)):\n",
    "            a = process[z]\n",
    "            for i,j in a.items():\n",
    "                if i in pr.keys():\n",
    "                    \n",
    "                    pr[i]= pr[i]+j\n",
    "                else:\n",
    "                    pr[i]=j\n",
    "    if case == 2:\n",
    "        pr = process[0]\n",
    "        for z in range(1,len(process)):\n",
    "            a = process[z]\n",
    "            for i,j in a.items():\n",
    "                if i in pr.keys():\n",
    "                    if pr[i] <= j:\n",
    "                        pr[i]=j\n",
    "                else:\n",
    "                    pr[i]=j\n",
    "        \n",
    "    search_final= sorted(pr.items(), reverse=True,key=lambda d: d[1]) \n",
    "    return search_final\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "# Search page GUI\n",
    "searchpage = Tk()\n",
    "searchpage.title(\"Hotel Search\")\n",
    "\n",
    "lab1 = Label(searchpage, text = \"IR Project Search Engine\")\n",
    "lab1.grid(row=0, column=1)\n",
    "\n",
    "lab2 = Label(searchpage, text = \"Result List\")\n",
    "lab2.grid(row=2, column=0)\n",
    "\n",
    "lab3 = Label(searchpage, text = \"Ranking List\")\n",
    "lab3.grid(row=2, column=2)\n",
    "\n",
    "inp = Entry(searchpage, width = 30)\n",
    "inp.grid(row=1, column=1)\n",
    "\n",
    "btn = Button(searchpage, text = \"Search\")\n",
    "btn.grid(row=2, column=1)\n",
    "btn.config(command = button_click)\n",
    "\n",
    "\n",
    "frm = Frame(searchpage)\n",
    "frm2 = Frame(searchpage)\n",
    "frm3 = Frame(searchpage)\n",
    "frm4 = Frame(searchpage)\n",
    "frm.grid(row=4, column=0, rowspan=2, sticky=N+S)\n",
    "frm2.grid(row=4, column=1, sticky=E+W+N)\n",
    "frm3.grid(row=5, column=1)\n",
    "frm4.grid(row=6,column=1)\n",
    "scrollbar = Scrollbar(frm, orient=\"vertical\")\n",
    "scrollbar.pack(side=RIGHT, fill=Y)\n",
    "scrollbar2 = Scrollbar(frm2, orient=\"horizontal\")\n",
    "scrollbar2.pack(side=BOTTOM, fill=X)\n",
    "scrollbar3 = Scrollbar(frm3, orient=\"vertical\")\n",
    "scrollbar3.pack(side=RIGHT, fill=Y)\n",
    "\n",
    "listNodes = Listbox(frm, width=60, yscrollcommand=scrollbar.set)\n",
    "listNodes.pack(expand=True, fill=Y)\n",
    "listNodes.bind('<<ListboxSelect>>',lambda event: reSelect(event))\n",
    "\n",
    "scrollbar.config(command=listNodes.yview)\n",
    "\n",
    "listSelection = Listbox(frm2, width=60, height=5, xscrollcommand=scrollbar2.set)\n",
    "listSelection.pack(expand=True, fill=X)\n",
    "\n",
    "scrollbar2.config(command=listSelection.xview)\n",
    "\n",
    "listRank = Listbox(searchpage, width=60)\n",
    "listRank.grid(row=4, column=2, rowspan=2, sticky=N+S)\n",
    "listRank.bind('<<ListboxSelect>>', lambda event: curSelect(event))\n",
    "\n",
    "\n",
    "clusterResult = Listbox(frm3, width=60, yscrollcommand=scrollbar3.set)\n",
    "clusterResult.pack(expand=True, fill=Y)\n",
    "\n",
    "scrollbar3.config(command=clusterResult.yview)\n",
    "\n",
    "listreview = tkscrolled.ScrolledText(frm4, wrap = tk.WORD, width = 60, height = 15)\n",
    "listreview.pack(expand=True, fill='both')\n",
    "\n",
    "\n",
    "compute_dataset()\n",
    "create_dictionary('pr_result.csv')\n",
    "searchpage.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Program.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
